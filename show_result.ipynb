{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5c6e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2ForXVector, AutoFeatureExtractor\n",
    "from sklearn.metrics import roc_curve, auc, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0fb09bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./finetuned_model/wav2vecXvectorModel_wav2vec2-base-960h/best_model.pth'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'VoiceDatasetWav2Vec'\n",
    "model_name = 'facebook/wav2vec2-base-960h'\n",
    "num_workers = 8\n",
    "batch_size = 24\n",
    "learning_rate =  0.00003\n",
    "warmup_steps_rate =  10\n",
    "seed_number = 42\n",
    "\n",
    "max_input_length = 4.0\n",
    "log_interval = 100\n",
    "\n",
    "data_path = '/workspace/ssd/AI_hub/speech_recognition/dataset/2.Validation/source_data/'\n",
    "\n",
    "wandb={'project_name': 'wav2vec_contrastive_learning','run_name': 'wav2vecXvectorModel'}\n",
    "\n",
    "save_path = f\"./finetuned_model/{wandb['run_name']}_{model_name.split('/')[-1]}/best_model.pth\"\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962ce2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fca1b5d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForXVector were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'feature_extractor.bias', 'feature_extractor.weight', 'objective.weight', 'projector.bias', 'projector.weight', 'tdnn.0.kernel.bias', 'tdnn.0.kernel.weight', 'tdnn.1.kernel.bias', 'tdnn.1.kernel.weight', 'tdnn.2.kernel.bias', 'tdnn.2.kernel.weight', 'tdnn.3.kernel.bias', 'tdnn.3.kernel.weight', 'tdnn.4.kernel.bias', 'tdnn.4.kernel.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForXVector(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2GroupNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2Encoder(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (tdnn): ModuleList(\n",
       "    (0): TDNNLayer(\n",
       "      (kernel): Linear(in_features=2560, out_features=512, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (1-2): 2 x TDNNLayer(\n",
       "      (kernel): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (3): TDNNLayer(\n",
       "      (kernel): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "    (4): TDNNLayer(\n",
       "      (kernel): Linear(in_features=512, out_features=1500, bias=True)\n",
       "      (activation): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (feature_extractor): Linear(in_features=3000, out_features=512, bias=True)\n",
       "  (classifier): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (objective): AMSoftmaxLoss(\n",
       "    (loss): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Wav2Vec2ForXVector.from_pretrained(model_name)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a50c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceDatasetWav2Vec(Dataset):\n",
    "    def __init__(self, data_path, max_length, model_name):\n",
    "        self.data_path = data_path\n",
    "        self.max_length = max_length\n",
    "        self.feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "        self.data = []\n",
    "    \n",
    "        for root, dirs, files in os.walk(self.data_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):\n",
    "                    self.data.append(os.path.join(root, file))\n",
    "                    \n",
    "        self.speaker_datas = {}\n",
    "        for file in self.data:\n",
    "            speaker_dir = os.path.dirname(file)\n",
    "            # speaker_dir = speaker_dir.split('/')[-1]\n",
    "            if speaker_dir not in self.speaker_datas:\n",
    "                self.speaker_datas[speaker_dir] = []\n",
    "            self.speaker_datas[speaker_dir].append(file)\n",
    "        \n",
    "\n",
    "    def load_audio(self, file_path):\n",
    "        audio, sampling_rate = librosa.load(file_path, sr=16000)\n",
    "        inputs = self.feature_extractor(audio, sampling_rate=sampling_rate, max_length=int(self.max_length * sampling_rate), truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        return inputs.input_values.squeeze(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anchor_data = self.data[idx]\n",
    "        anchor_audio = self.load_audio(anchor_data)\n",
    "        \n",
    "        speaker_dir = os.path.dirname(anchor_data)\n",
    "        speaker_datas = sorted(self.speaker_datas[speaker_dir])\n",
    "        \n",
    "        anchor_idx = speaker_datas.index(anchor_data)\n",
    "        positive_idx = (anchor_idx + 1) % len(speaker_datas)\n",
    "        positive_data = speaker_datas[positive_idx]\n",
    "        positive_audio = self.load_audio(positive_data)\n",
    "        \n",
    "        negative_datas_dirs = [d for d in self.speaker_datas.keys() if d != speaker_dir]\n",
    "        negative_speaker_dir = random.choice(negative_datas_dirs)\n",
    "        negative_data = random.choice(self.speaker_datas[negative_speaker_dir])\n",
    "        negative_audio = self.load_audio(negative_data)\n",
    "\n",
    "        return anchor_audio, positive_audio, negative_audio\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afab9967",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = VoiceDatasetWav2Vec(data_path, 4.0, model_name)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c11fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(embedding1, embedding2):\n",
    "    return F.cosine_similarity(embedding1, embedding2).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7342588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "similarities = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03fa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30598 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "  0%|          | 117/30598 [00:16<1:11:31,  7.10it/s]"
     ]
    }
   ],
   "source": [
    "valid_progress_bar = tqdm(valid_dataloader)\n",
    "for idx, batch in enumerate(valid_progress_bar):\n",
    "    anchor, positive, negative = batch\n",
    "    anchor = anchor.to(device)\n",
    "    positive = positive.to(device)\n",
    "    negative = negative.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        anchor_embeddings = model(anchor).embeddings\n",
    "        positive_embeddings = model(positive).embeddings\n",
    "        negative_embeddings = model(negative).embeddings\n",
    "\n",
    "    pos_similirarity = calculate_similarity(anchor_embeddings, positive_embeddings)\n",
    "    neg_similirarity = calculate_similarity(anchor_embeddings, negative_embeddings)\n",
    "\n",
    "    similarities.extend(pos_similirarity)\n",
    "    similarities.extend(neg_similirarity)\n",
    "    labels.extend([1] * len(pos_similirarity))\n",
    "    labels.extend([0] * len(neg_similirarity))\n",
    "\n",
    "# Calculate ROC AUC\n",
    "fpr, tpr, thresholds = roc_curve(labels, similarities)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Calculate Youden's J statistic to find the optimal threshold\n",
    "J = tpr - fpr\n",
    "ix = np.argmax(J)\n",
    "best_threshold = thresholds[ix]\n",
    "\n",
    "# Calculate F1 score at the optimal threshold\n",
    "binary_predictions = (similarities > best_threshold).astype(int)\n",
    "f1 = f1_score(labels, binary_predictions)\n",
    "\n",
    "print(f'ROC AUC: {roc_auc:.4f}, Best Threshold: {best_threshold:.4f}, F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582d37c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
